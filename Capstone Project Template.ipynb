{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "USA is the land of opportunities, and one of most (if not the most) required destinations of immigrants.\n",
    "year after year, USA is providing opportunities like lottery of green card, work permits and students visa.\n",
    "\n",
    "One of demands of american governments is regulating and tuning the immigration according to USA needs.\n",
    "\n",
    "Project in hands is collecting related data sets into a simple data ware house where analysts can investigate and come up with insights about immigration and different factors those can affect it.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import expr , udf ,trim ,year, month, dayofmonth\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import configparser\n",
    "\n",
    "# CONFIG\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "DWH_ROLE_ARN           = config.get('IAM_ROLE','ARN')\n",
    "S3_bucket              = config.get('S3','S3_bucket')\n",
    "accesskey              = config.get('S3','accesskey')\n",
    "secretkey              = config.get('S3','secretkey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "Current project is dealing mainly with immigration data comming from USA immigration data set as primary source of data beside 3 other data sets including temprature, cities and airports.\n",
    "there are also small dictionary tables found will help us to explain data, like countries and states.\n",
    "\n",
    "#### Plan \n",
    "there are many alternatives for processing data in this project. 2 of them will be explained in details\n",
    "\n",
    "1. Spark data frames and Spark SQL : \n",
    "    1. reading system files (csv and parquet) using Spark Data Frame\n",
    "    2. Generating data files from existing dictionary attached.\n",
    "    3. Data Quality using Spark Data frame and Spark SQL\n",
    "    4. Generating star model schema from data sets and dictionaries.\n",
    "    5. Storing new generated schema to csv or parquet files.\n",
    "    \n",
    "    \n",
    "2. S3 and Redshift :\n",
    "    1. Uploading data to S3 bucket\n",
    "    2. Loading data from S3 to Redshift.\n",
    "    3. Data Quality for raw data sets.\n",
    "    4. Generating Star Model for new tables in redshift\n",
    "    5. Data Quality on generated data in new tables.   \n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "   \n",
    "1. I94 Immigration Data : data set coming from **US National Tourism and Trade Office** it contains data about immigrants {like year of birth, nationality, ..} beside their visa info {arrival, departure, type ...} and which airport they came through.\n",
    "\n",
    "2. World Temperature Data: This dataset came from **Kaggle** , it includes average temprature for city over number of years.\n",
    "\n",
    "3. U.S. City Demographic Data: from **OpenSoft**. it contains city name, state in addition to some statistics about its population.\n",
    "\n",
    "4. Airport Code Table: a simple table of airport codes, it can be generated from **IATA** or other sites related to flight regulations.\n",
    "\n",
    "\n",
    "| Data Set | Size in MB |  Number of rows |\n",
    "|----------|:-------------:|------:|\n",
    "| Immigration |  6000 | nearly 1,600,000  |\n",
    "| Temprature |  509|  8,599,213|  \n",
    "| Airports |  6|  55,075| \n",
    "| Cities |  0.25|  2,891| \n",
    "\n",
    "in addition to dictionaries from attached files\n",
    "\n",
    "| Data Set | Data Set  |  Columns |\n",
    "|----------|:-------------:|------:|\n",
    "| Countries |  Immigration  | i94cit, i94res  |\n",
    "| states |  Immigration and airports|  i94addr , state|  \n",
    "| ports |  immigration |  i94port| \n",
    "| Visa model |  immigration|  i94mod| \n",
    "| Visa  |  immigration|  i94visa|\n",
    "\n",
    "#### Use Cases\n",
    "resulting data model will be supporting data analytics team to answer many questions like the following\n",
    "\n",
    "1. what is the ratio of different types of visas granted (student , pleasure , Work ) ?\n",
    "2. which countries are the biggest source of immigrants?\n",
    "3. the ratio of Foreign-born for every city / state, and which states have the biggest ratio of foreign-born vs. locals?\n",
    "4. which states are preferred for immigrants (student , work , pleasure) ?\n",
    "5. what are the range of ages for immigrants ?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_spark.write.parquet(\"sas_data\")\n",
    "df_spark=spark.read.parquet(\"sas_data\")\n",
    "df_spark.createOrReplaceTempView('immigrationraw')\n",
    "spark.sql(''' select * from immigrationraw limit 10''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "\n",
    "raw data sets are including a lot of issues but important data still could be retrieved from it.\n",
    "\n",
    "1. Immigration Data set includes the following issues:\n",
    "    1. many columns contain null values, but most important are:\n",
    "        occup: the occupation of immigrant\n",
    "        i94addr: the state where immigrant will reside.\n",
    "        gender\n",
    "        \n",
    "        while other columns are logical to have null, like DEPTDATE, it's logical if immigrant still inside USA , or even if he didn't leave and stayed illegally.\n",
    "        \n",
    "    2. this data set contains many formats of dates, one of them is **SAS date** format, which is the number of days since 1/1/1960\n",
    "        some date formats are **yyyyMMdd** and some are **MMddyyyy** which is strange to be in the same data set.\n",
    "    3. a lot of codes those are meaning less, and needs to be interpreted with the attached dictionary, like **i94cit** which is the citizenship of immigrants.\n",
    "        and **i94res** which is the source country of issuing visa.\n",
    "        \n",
    "2. Airports: although should be accurate, it contains a lot of nulls in **iata_code**, this is why it can't match except 251 value of 299 airport existing in immigration data set\n",
    "3. Cities : sounds very organized data set , although the summation of different races counts are greated than the total number for most of cities.\n",
    "4. Temprature: very huge data set contains temprature degrees since before 1800 which is useless to our case, so data will be filtered based on date.\n",
    "    \n",
    "#### Cleaning Steps\n",
    "\n",
    "main data set is immigration, so most of effort was to clean this set and make it readable.\n",
    "1- SAS date was transformed to date type\n",
    "    ```\n",
    "    immigrationDf = immigrationDf.withColumn('arrdate2', expr(\"date_add(to_date('1960-01-01'),arrdate)\"))\n",
    "    immigrationDf = immigrationDf.withColumn('depdate2', expr(\"date_add(to_date('1960-01-01'),depdate)\"))\n",
    "    ```\n",
    "2- String columns with different date formats are transferred to Date\n",
    "    ```\n",
    "    immigrationDf = immigrationDf.withColumn('dtadfile2', expr(\"to_date(dtadfile ,'yyyyMMdd')\"))\n",
    "    immigrationDf = immigrationDf.withColumn('dtaddto2', expr(\"to_date(dtaddto,'MMddyyyy')\"))\n",
    "    ```\n",
    "3- for ambiguous columns, all related data are retrieved, dictionary tables are created to interpret vague columns like \n",
    "    i94res, i94cit , i94mod, i94visa\n",
    "4- Dictionary files contains a lot of problems where it didn't comply directly to schema because of missing separators, or extra separators \n",
    "    a lot of manual corrections done to extract data to be useful in our data model.\n",
    "    final dictionary files are attached in dictionary folder.\n",
    "5- **biggest problem** in data of airports are not matching completely, from 299 airport mentioned in immigration data set, only 251 matching airport in airports data set by iata_code. and not all cities are matching with cities data set.\n",
    "i tried with ports dictionary, although it matched 100% of airports as iata_code but a lot of airports are missing the cities.\n",
    "\n",
    "so i used external data source from iata, i had in data analysis nano degree project.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### Before data modeling\n",
    "from attached SAS_Labes_description, i generated 5 csv files, all of them will be used while pipelining raw data into formatted star model schema.\n",
    "these files are :\n",
    "1. countries.csv\n",
    "2. states.csv\n",
    "3. ports.csv\n",
    "4. visaModel.csv\n",
    "5. visa.csv\n",
    "\n",
    "the file createRawSchema.sql can be used to create raw tables in Redshift.\n",
    "\n",
    "and the following graph represents the schema of raw data sets beside dictionary tables those helping interpret data.\n",
    "<img src=\"graphs/rawSchema.png\">\n",
    "\n",
    "the below code is used to transfer file from S3 to redshift raw schema after uploading data files to s3bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy tempratureraw from S3_bucket+'GlobalLandTemperaturesByCity.csv'\n",
    "ACCESS_KEY_ID accesskey\n",
    "SECRET_ACCESS_KEY secretkey\n",
    "format as csv \n",
    "IGNOREHEADER 1;\n",
    "\n",
    "copy citiesraw from S3_bucket+'us-cities-demographics.csv'\n",
    "ACCESS_KEY_ID accesskey\n",
    "SECRET_ACCESS_KEY secretkey\n",
    "format as csv \n",
    "IGNOREHEADER 1 delimiter ';';\n",
    "\n",
    "copy airportsraw from S3_bucket+'airport-codes_csv.csv'\n",
    "ACCESS_KEY_ID accesskey\n",
    "SECRET_ACCESS_KEY secretkey\n",
    "format as csv \n",
    "IGNOREHEADER 1;\n",
    "\n",
    "copy immigrationraw from S3_bucket+'immigrationparquet'\n",
    "iam_role DWH_ROLE_ARN\n",
    "format as parquet ;\n",
    "\n",
    "copy countriesdic from S3_bucket+'countries.csv'\n",
    "ACCESS_KEY_ID accesskey\n",
    "SECRET_ACCESS_KEY secretkey\n",
    "format as csv \n",
    "IGNOREHEADER 1;\n",
    "\n",
    "copy statesdic from S3_bucket+'states.csv'\n",
    "ACCESS_KEY_ID accesskey\n",
    "SECRET_ACCESS_KEY secretkey\n",
    "format as csv \n",
    "IGNOREHEADER 1;\n",
    "\n",
    "copy portsdic from S3_bucket+'ports.csv'\n",
    "ACCESS_KEY_ID accesskey\n",
    "SECRET_ACCESS_KEY secretkey\n",
    "format as csv \n",
    "IGNOREHEADER 1;\n",
    "\n",
    "copy visamodeldic from S3_bucket+'VisaModel.csv'\n",
    "ACCESS_KEY_ID accesskey\n",
    "SECRET_ACCESS_KEY secretkey\n",
    "format as csv \n",
    "IGNOREHEADER 1;\n",
    "\n",
    "copy visadic from S3_bucket+'visa.csv'\n",
    "ACCESS_KEY_ID accesskey\n",
    "SECRET_ACCESS_KEY secretkey\n",
    "format as csv \n",
    "IGNOREHEADER 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Pre Model\n",
    "the following code creates temporary tables from given data files and the gnerated dictionary files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempratureDf = spark.read.csv('../../GlobalLandTemperaturesByCity.csv',header=True, inferSchema=True) .where(year('dt')==2013).limit(1000)\n",
    "tempratureDf.printSchema()\n",
    "tempratureDf.createOrReplaceTempView(\"tempratureraw\")\n",
    "tempratureDf.show()\n",
    "\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigrationDf=spark.read.parquet(fname)\n",
    "\n",
    "immigrationDf.createOrReplaceTempView(\"immigrationraw\")\n",
    "immigrationDf.write.parquet(\"immigration2\")\n",
    "immigrationDf = immigrationDf.withColumn('arrdate2', expr(\"date_add(to_date('1960-01-01'),arrdate)\"))\n",
    "immigrationDf = immigrationDf.withColumn('depdate2', expr(\"date_add(to_date('1960-01-01'),depdate)\"))\n",
    "immigrationDf = immigrationDf.withColumn('dtadfile2', expr(\"to_date(dtadfile ,'yyyyMMdd')\"))\n",
    "immigrationDf = immigrationDf.withColumn('dtaddto2', expr(\"to_date(dtaddto,'MMddyyyy')\"))\n",
    "immigrationDf.printSchema()\n",
    "immigrationDf.show()\n",
    "\n",
    "airportsDf = spark.read.csv(\"airport-codes_csv.csv\", header=True, inferSchema=True)\n",
    "airportsDf.printSchema()\n",
    "airportsDf.createOrReplaceTempView('airportsraw')\n",
    "airportsDf.show()\n",
    "\n",
    "portsDf = spark.read.csv(\"ports.csv\", header=True, inferSchema=True)\n",
    "portsDf.printSchema()\n",
    "portsDf.createOrReplaceTempView('portsraw')\n",
    "portsDf.show()\n",
    "\n",
    "citiesDf=spark.read.csv(\"us-cities-demographics.csv\",sep=';',header=True, inferSchema=True)\n",
    "citiesDf.printSchema()\n",
    "citiesDf.createOrReplaceTempView('citiesraw')\n",
    "citiesDf.show()\n",
    "\n",
    "statesDf = spark.read.csv(\"states.csv\",  header=True, inferSchema=True)\n",
    "statesDf.printSchema()\n",
    "statesDf.createOrReplaceTempView('statesraw')\n",
    "statesDf.show()\n",
    "\n",
    "countriesDf = spark.read.csv(\"countries.csv\", header=True, inferSchema=True)\n",
    "countriesDf.printSchema()\n",
    "countriesDf.createOrReplaceTempView('countriesraw')\n",
    "countriesDf.show()\n",
    "\n",
    "visamodelDf = spark.read.csv(\"VisaModel.csv\",header=True, inferSchema=True)\n",
    "visamodelDf.printSchema()\n",
    "visamodelDf.createOrReplaceTempView('visamodelraw')\n",
    "visamodelDf.show()\n",
    "\n",
    "visaDf = spark.read.csv(\"Visa.csv\", header=True, inferSchema=True)\n",
    "visaDf.printSchema()\n",
    "visaDf.createOrReplaceTempView('visaraw')\n",
    "visaDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "I generated Star Data Model from the raw data sets.\n",
    "\n",
    "it''s most suitable model for our case.\n",
    "apparently there are 4 main business entities here\n",
    "1. Immigrants data.\n",
    "2. Visa transactions\n",
    "3. cities in united states.\n",
    "4. temprature data  \n",
    "\n",
    "so the data model will consist of 3 dimensions\n",
    "1. temprature\n",
    "2. city\n",
    "3. immigrants\n",
    "\n",
    "and one facts table visa_transactions\n",
    "\n",
    "Hence we are working with 2 methodoligies in parallel, the following section will explain both of them:\n",
    "\n",
    "##### Spark Conceptual Data Model.\n",
    "we will work with this model in generated temporary views.\n",
    "##### Redshift Conceptual Data Model\n",
    "in redshift, we will have similar data model, except that table optimization is considered for performance improvement.\n",
    "so tables cities is considered to be All diststyle. because it's limited data set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create table if not exists temprature(\n",
    "    dt timestamp NOT NULL,\n",
    "    AverageTemperature int4 NOT NULL,\n",
    "    AverageTemperatureUncertainty varchar(256),\n",
    "    cityid int ,\n",
    "    primary key(dt, cityid) distkey\n",
    " );\n",
    "   \n",
    "create table if not exists city\n",
    "(\n",
    "  cityid int primary key,\n",
    "  City  varchar(256) ,\n",
    "  State  varchar(256) ,\n",
    "  Median_Age  real ,\n",
    "  Male_Population  integer ,\n",
    "  Female_Population  integer ,\n",
    "  Total_Population  integer ,\n",
    "  Number_of_Veterans  integer ,\n",
    "  Foreign_born  integer ,\n",
    "  Average_Household_Size  real ,\n",
    "  State_Code  varchar(256) ,\n",
    "  White  int ,\n",
    "  Asian int ,\n",
    "  Hispanic_or_Latino int,\n",
    "  Black_or_African_American int ,\n",
    "  American_Indian_and_Alaska_Native int\n",
    ") diststyle all;\n",
    "\n",
    "create table if not exists immigrants\n",
    "(\n",
    "  id  int primary key distkey,  \n",
    "  citizen  varchar(256) ,\n",
    "  resource  varchar(256) ,\n",
    "  addr  varchar(256) ,\n",
    "  age  real ,\n",
    "  count  real ,\n",
    "  occup  varchar(256) ,\n",
    "  birthyear  int ,\n",
    "  gender  varchar(256) \n",
    ");\n",
    "create table if not exists visa_transactions(\n",
    "  id  int primary key distkey,\n",
    "  year  int ,\n",
    "  month  int ,\n",
    "  port  varchar(256) ,\n",
    "  arrdate  date ,\n",
    "  model  varchar(256) ,\n",
    "  depdate  date ,\n",
    "  visa  varchar(256) ,\n",
    "  visapost  varchar(256) ,\n",
    "  entdepa  varchar(256) ,\n",
    "  entdepd  varchar(256) ,\n",
    "  entdepu  varchar(256) ,\n",
    "  matflag  varchar(256) ,\n",
    "  dtaddto  varchar(256) ,\n",
    "  insnum  varchar(256)  ,\n",
    "  airline  varchar(256) ,\n",
    "  admnum  real ,\n",
    "  fltno  varchar(256) ,\n",
    "  visatype  varchar(256) \n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "after we loaded data successfully into raw tables; using Spark data frame or from S3 to Redshift, remaining task is quite straight forward, we can simply load data by joining necessary raw tables with dictionaries as explained in the following section.\n",
    "\n",
    "this graph shows the resulting star model\n",
    "<img src=\"graphs/StarDataModel.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "the following are the main queries required to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select i.cicid id ,i.i94yr year,i.i94mon month,i.i94port port,i.arrdate,vm.type model,i.depdate,v.type visa,i.visapost,i.entdepa,i.entdepd ,i.entdepu ,i.matflag ,i.dtaddto ,i.insnum ,i.airline ,i.admnum ,i.fltno ,i.visatype \n",
    "    from immigrationraw i join visamodeldic vm on i.i94mode =vm.code  \n",
    "                          join visadic v on i.i94visa = v.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select i.cicid id ,c.country citizen, c2.country resource,s.state addr,i.i94bir age ,i.count , i.occup ,i.biryear birthyear, i.gender  \n",
    "  from immigrationraw i join countriesdic c on i.i94cit =c.code\n",
    "  join countriesdic c2 on i.i94res =c2.code\n",
    "  join statesdic s on i.i94addr = s.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select a.City , a.State, a.`Median Age` Median_Age , a.`Male Population` Male_Population , a.`Female Population` Female_Population, \n",
    "         a.`Total Population` Total_Population, a.`Number of Veterans` Number_of_Veterans, a.`Foreign-born` Foreign_born, a.`Average Household Size` Average_Household_Size , \n",
    "         a.`State Code` State_Code,a.count \n",
    "         ,nvl(b.count,0) ,nvl(c.count,0),nvl(d.count,0),nvl(e.count,0) \n",
    "  from citiesraw a left join citiesraw b on a.City=b.City and a.State =b.State \n",
    "                left join citiesraw c on a.City=c.City and a.State =c.State\n",
    "                left join citiesraw d on a.City=d.City and a.State =d.State\n",
    "                left join citiesraw e on a.City=e.City and a.State =e.State\n",
    "             where a.Race='White' and b.Race='Asian' and c.Race='Hispanic or Latino' and d.Race='Black or African-American' and e.Race='American Indian and Alaska Native'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "visatransactions=spark.sql(''' select i.cicid id ,i.i94yr year,i.i94mon month,i.i94port port,i.arrdate,vm.type model,i.depdate,v.type visa,i.visapost,i.entdepa,i.entdepd ,i.entdepu ,i.matflag ,i.dtaddto ,i.insnum ,i.airline ,i.admnum ,i.fltno ,i.visatype \n",
    "from immigrationraw i join visamodeldic vm on i.i94mode =vm.code  join visadic v on i.i94visa = v.code ''')\n",
    "visatransactions.show()\n",
    "visatransactions.createOrReplaceTempView(\"visatransactions\")\n",
    "\n",
    "immigrants=spark.sql('''select i.cicid id ,c.country citizen, c2.country resource,s.state addr,i.i94bir age ,i.count , i.occup ,i.biryear birthyear, i.gender  \n",
    "          from immigrationraw i join countriesdic c on i.i94cit =c.code\n",
    "          join countriesdic c2 on i.i94res =c2.code\n",
    "          join statesdic s on i.i94addr = s.code''')\n",
    "immigrants.show()\n",
    "immigrants.createOrReplaceTempView(\"immigrants\")\n",
    "\n",
    "cities=spark.sql(''' select a.City , a.State, a.`Median Age` Median_Age , a.`Male Population` Male_Population , a.`Female Population` Female_Population, \n",
    "     a.`Total Population` Total_Population, a.`Number of Veterans` Number_of_Veterans, a.`Foreign-born` Foreign_born, a.`Average Household Size` Average_Household_Size , \n",
    "     a.`State Code` State_Code,a.count \n",
    "     ,nvl(b.count,0) ,nvl(c.count,0),nvl(d.count,0),nvl(e.count,0) \n",
    "      from citiesraw a left join citiesraw b on a.City=b.City and a.State =b.State \n",
    "                    left join citiesraw c on a.City=c.City and a.State =c.State\n",
    "                    left join citiesraw d on a.City=d.City and a.State =d.State\n",
    "                    left join citiesraw e on a.City=e.City and a.State =e.State\n",
    "                 where a.Race='White' and b.Race='Asian' and c.Race='Hispanic or Latino' and d.Race='Black or African-American' and e.Race='American Indian and Alaska Native'\n",
    "        ''')\n",
    "cities.show()\n",
    "cities.createOrReplaceTempView(\"cities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "spark.sql(''' select count(*) from immigrationraw''').show()\n",
    "spark.sql('''select count(distinct City) from citiesraw''').show()\n",
    "\n",
    "spark.sql(''' select count(*) from visatransactions''').show()\n",
    "spark.sql(''' select count(*) from immigrants''').show()\n",
    "spark.sql(''' select count(*) from cities''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* number of records in immigration raw data set. : 3,096,313\n",
    "* number of records in visatransactions fact table is : 3,096,074\n",
    "\n",
    "difference is due to some missing data in i94mod and i94visa\n",
    "which can be got by this query\n",
    "```\n",
    "spark.sql(''' select count(*) from immigrationraw where i94mode is null or i94visa is null''').show()\n",
    "```\n",
    "resulting in 239\n",
    "\n",
    "but much bigger discrepancy found between number of immigrants and original raw immigration data set, because a lot of nulls in i94addr which is the state where the immigrant will reside in\n",
    "\n",
    "```\n",
    "spark.sql(''' select count(*) from immigrationraw where i94res is null or i94cit is null or i94addr is null''').show()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "|table_name|column_name|?column?|?column?|\n",
    "|----------|-----------|--------|--------|\n",
    "|temprature|dt|tempratureraw|time stamp of temprature reading|\n",
    "|temprature|averagetemperature|tempratureraw| average temprate read|\n",
    "|temprature|averagetemperatureuncertainty|tempratureraw|certainty of temprature sensor accuracy|\n",
    "|temprature|cityid|tempratureraw| city where the temprature was measured|\n",
    "| | | | |\n",
    "|city|cityid|citiesraw| city id |\n",
    "|city|city|citiesraw|city name |\n",
    "|city|state|citiesraw| state of city|\n",
    "|city|median_age|citiesraw| median age of population in city|\n",
    "|city|male_population|citiesraw| number of males in city|\n",
    "|city|female_population|citiesraw| number of females in city|\n",
    "|city|total_population|citiesraw| total number of population in city|\n",
    "|city|number_of_veterans|citiesraw| number of experienced persons in city|\n",
    "|city|foreign_born|citiesraw| number of persons born outside USA|\n",
    "|city|average_household_size|citiesraw| the average number of family|\n",
    "|city|state_code|citiesraw| sate code for the sate city belongs to|\n",
    "|city|white|citiesraw| number of population of white race in city|\n",
    "|city|asian|citiesraw|number of population of asian race in city|\n",
    "|city|hispanic_or_latino|citiesraw|number of population of hispanic or latino race in city|\n",
    "|city|black_or_african_american|citiesraw|number of population of black or african american race in city|\n",
    "|city|american_indian_and_alaska_native|citiesraw|number of population of american indian or alaska native race in city|\n",
    "| | | | |\n",
    "|immigrants|id|immigrationraw| id for immigrant|\n",
    "|immigrants|citizen|countriesraw| immigrant citizenship, the equivalent country name for code in immigrationraw.i94cit|\n",
    "|immigrants|resource|immigrationraw|immigrant country when applied for visa, the equivalent country name for code in immigrationraw.i94res|\n",
    "|immigrants|addr|immigrationraw| state code|\n",
    "|immigrants|age|immigrationraw| age of immigrant|\n",
    "|immigrants|count|immigrationraw| number of visas|\n",
    "|immigrants|occup|immigrationraw| occupation of the immigrant|\n",
    "|immigrants|birthyear|immigrationraw| birth year of the immigrant|\n",
    "|immigrants|gender|immigrationraw|gender of immigrant|\n",
    "| | | | |\n",
    "|visa_transactions|id|immigrationraw| id for visa transaction|\n",
    "|visa_transactions|year|immigrationraw| year of crossing border |\n",
    "|visa_transactions|month|immigrationraw| month of crossing border|\n",
    "|visa_transactions|port|immigrationraw| port of entrance|\n",
    "|visa_transactions|arrdate|immigrationraw| arrival date|\n",
    "|visa_transactions|model|immigrationraw| visa model air,sea,land, other|\n",
    "|visa_transactions|depdate|immigrationraw| leaving of country|\n",
    "|visa_transactions|visa|immigrationraw| visa type , busines or student or pleasure |\n",
    "|visa_transactions|visapost|immigrationraw| visa admission office in foreign country|\n",
    "|visa_transactions|entdepa|immigrationraw|Arrival Flag - admitted or paroled into the U.S.|\n",
    "|visa_transactions|entdepd|immigrationraw|Departure Flag - Departed, lost I-94 or is deceased|\n",
    "|visa_transactions|entdepu|immigrationraw|Update Flag - Either apprehended, overstayed, adjusted to perm residence|\n",
    "|visa_transactions|matflag|immigrationraw|Match flag - Match of arrival and departure records |\n",
    "|visa_transactions|dtaddto|immigrationraw| Character Date Field - Date to which admitted to U.S. (allowed to stay until)|\n",
    "|visa_transactions|insnum|immigrationraw| INS number|\n",
    "|visa_transactions|airline|immigrationraw| airline name of immigrant flight|\n",
    "|visa_transactions|admnum|immigrationraw| admission serian number|\n",
    "|visa_transactions|fltno|immigrationraw| flight number |\n",
    "|visa_transactions|visatype|immigrationraw| visa category |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "## Goal of project\n",
    "final output from this data engineering project was to clean and transform data into analytical model that is it's ready to be analyzed with sql queries to extract patterns and insights by data science team.\n",
    "\n",
    "we showed 2 methodologies for loading data: -\n",
    "\n",
    "with Spark Data frames and SQL, resulting model was tables those can be queries with Spark sql.\n",
    "with S3 and redshift, resulting similar star model in redshift.\n",
    "\n",
    "## Queries could be run.\n",
    "\n",
    "## Tools used.\n",
    "* spark data frames and spark sql\n",
    "* S3 and redshift\n",
    "* (optional) Airflow\n",
    "\n",
    "given data set is considered small (about 6 Giga bytes) and almost fully structured.\n",
    "6GB can fit with single machine memory , so using Spark data frames and spark sql will achieve the goal of project.\n",
    "\n",
    "to use cloud technologies, we could store data in S3 buckets and process it using Redshift cluster.\n",
    "redshift gives us a lot of flexibility and better performance especialy with using correct optimization when creating dimensions and Fact tables.\n",
    "\n",
    "Airflow could be used to schedule loading data from raw datasets to data ware house model.\n",
    "not a lot of operators required, mainly S3 to Redshift operator.\n",
    "other postgres operators may be needed to run queries for reporting or dashboard.\n",
    "\n",
    "## Model choice\n",
    "there are many alternatives for data modeling in our project, we could choose either star data model or snow-flake model.\n",
    "snow-flake is more relevant espacially with the hierarchy of demographic data such as {airport >> city >> state }\n",
    "but it will be more distracting especially there is no more data for states for example, so to make it tidy model, star data model was adopted in project.\n",
    "\n",
    "it's clear that few business entities are embedded in given raw data sets, as discussed before.\n",
    "dictionary data are used directly to interpret ambigous columns instead of doing many references.\n",
    "\n",
    "#### Data update schedule.\n",
    "about 3 million visa transactions happens per year, so, it will be a good practice to load this data daily.\n",
    "one could suggest best hour for loading data with least load, but dates included contains no information about the exact time of transaction.\n",
    "\n",
    "#### Github repository \n",
    "https://github.com/engomar2003/UdacityDataEngineeringCapstoneProject.git\n",
    "\n",
    "\n",
    "#### Changes required on solution if data changed by the following factors:\n",
    "* **The data was increased by 100x.**\n",
    "\n",
    "     current data size is about 6GB for immigration dataset. 0.5 GB for tempratures data set, few MBs for other data sets.\n",
    "     if the data increases 100 times (approaching 1 Tera), reserving big space in Redshift cluster will be expensive,\n",
    "     also dealing with it using Spark SQL will be over-utilizing memories and may lead to memory overflow crash.\n",
    "     \n",
    "     best solution is using EMR (Spark +S3) or EMR (serverless).\n",
    "     in this case we use S3 as reliable cheap storage, and with Spark or lambdas , we can perform the needed processing.\n",
    "     \n",
    "     also note: S3 bucket doesn't allow uploading files over 160GB in direct way, so it will suggest using:\n",
    "     **AWS CLI, AWS SDK, or Amazon S3 REST API**\n",
    "     or Airflow should be used for data loading and transformation, with partitioned data by month, to increase performance.\n",
    "     \n",
    "     \n",
    "* **The data populates a dashboard that must be updated on a daily basis by 7am every day.**\n",
    "\n",
    "     in this case we use Airflow to load data and execute needed queries for dashboard per agreed schedule.\n",
    "     or we can use EMR serverless, and define schedule for running lambdas.\n",
    "     in both cases, it's better to use S3 for storage as the data is required to be processed only at short time.\n",
    "     this will be more efficient. money-wise.\n",
    "     \n",
    "* **The database needed to be accessed by 100+ people.**\n",
    "\n",
    "     in this case, data will be accessed a lot and over a long period of time.\n",
    "     best practice will be using Redshift cluster or EMR with HDFS+Spark (if larger size)   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
